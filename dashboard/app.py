import streamlit as st
import numpy as np
import sys
import os

# --- Pfad zu src hinzuf√ºgen ---
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.load_model import load_model_and_data
from src.fairness_metrics import group_accuracy
from src.bias_analysis import compute_bias
from src.report_generator import generate_report

# NEU: aktualisierte Text-Bias-Funktionen
from src.text_bias_analysis import run_text_bias_test, GROUP_PROMPTS

import pandas as pd
import altair as alt
import matplotlib.pyplot as plt


# --- Dashboard Titel ---
st.title("AI Ethics Skill-Profiler Dashboard")
st.write("Analyse von Fairness & Bias in einem TensorFlow-Modell")


# --- Tabs ---
tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
    "Klassifikations-Bias",
    "Text-Bias (LSTM)",
    "Bild-Bias Kategorien",
    "Bild-Bias Analyse",
    "NLP Bias Scanner",
    "Edge AI Robustness & Bias Simulator",
    "AI Ethics Live Monitor"
])




# ================================================================
# TAB 1: Klassifikations-Fairness (unver√§ndert)
# ================================================================
with tab1:

    st.header("Klassifikations-Bias Analyse")

    if st.button("üîç Analyse starten"):

        # 1) Modell + Daten laden
        model, X_test, y_test, groups = load_model_and_data()

        # 2) Vorhersagen
        y_pred_prob = model.predict(X_test)
        y_pred = np.argmax(y_pred_prob, axis=1)

        # 3) Gruppen-Genauigkeit
        group_acc = group_accuracy(y_test, y_pred, groups)

        # 4) Bias Score
        bias = compute_bias(group_acc)

        # 5) Report generieren
        report = generate_report(group_acc, bias)

        # --- Anzeige im Dashboard ---
        st.subheader("Ergebnisse")

        st.write("### Gruppen-Genauigkeiten")
        st.json(group_acc)

        # ======================================================
        # üö¶ Ampel-Barchart
        # ======================================================
        df = pd.DataFrame({
            "Gruppe": list(group_acc.keys()),
            "Accuracy": list(group_acc.values())
        })

        def accuracy_color(acc):
            if acc < 0.5:
                return "red"
            elif acc < 0.8:
                return "orange"
            return "green"

        df["Color"] = df["Accuracy"].apply(accuracy_color)

        chart = (
            alt.Chart(df)
            .mark_bar(size=50)
            .encode(
                x=alt.X("Gruppe:N", title="Gruppe"),
                y=alt.Y("Accuracy:Q", title="Genauigkeit", scale=alt.Scale(domain=[0, 1])),
                color=alt.Color("Color:N", scale=None, legend=None)
            )
        )

        st.write("### Gruppen-Genauigkeit (Diagramm)")
        st.altair_chart(chart, use_container_width=True)
        # ======================================================

        st.write(f"### Bias Score: **{bias:.2f}**")

        if bias > 0.10:
            st.error("‚ö†Ô∏è Das Modell ist m√∂glicherweise unfair.")
        else:
            st.success("‚úîÔ∏è Das Modell wirkt fair.")

        st.write("### Report")
        st.code(report)

    else:
        st.info("Klicke auf 'Analyse starten', um die Fairness zu pr√ºfen.")



# ================================================================
# TAB 2: TEXT-BIAS (LSTM) ‚Äì NEU, ERWEITERT & KORRIGIERT
# ================================================================
with tab2:

    st.header("Text-Bias Analyse (LSTM)")
    st.write("Das Modell generiert Texte zu verschiedenen Gruppen. "
             "Wir analysieren systematisch Sentiment & Toxicity.")


    # --- Parameter f√ºr die Analyse ---
    n_samples = st.slider("Texte pro Gruppe", 3, 20, 5)
    max_words = st.slider("Max. Wortanzahl pro Text", 5, 50, 20)


    if st.button("üîç Text-Bias Analyse starten"):

        with st.spinner("Generiere Texte und analysiere Bias..."):
            results = run_text_bias_test(
                n_samples_per_group=n_samples,
                words_per_sample=max_words
            )

        group_results = results["group_results"]
        bias_scores = results["bias"]


        # -----------------------------------------------------
        # √úbersichtstabelle
        # -----------------------------------------------------
        st.subheader("Ergebnisse pro Gruppe")

        table_data = []
        for group, data in group_results.items():
            table_data.append({
                "Gruppe": group,
                "Sentiment (√ò)": data["sentiment_avg"],
                "Toxicity (√ò)": data["toxicity_avg"],
                "Texte": data["n_texts"]
            })

        df_bias = pd.DataFrame(table_data)
        st.dataframe(df_bias, use_container_width=True)



        # -----------------------------------------------------
        # Diagramm: Sentiment
        # -----------------------------------------------------
        st.write("### Sentiment-Vergleich zwischen Gruppen")

        fig1, ax1 = plt.subplots()
        ax1.bar(df_bias["Gruppe"], df_bias["Sentiment (√ò)"])
        ax1.set_xlabel("Gruppe")
        ax1.set_ylabel("Durchschnittliches Sentiment")
        st.pyplot(fig1)



        # -----------------------------------------------------
        # Diagramm: Toxicity
        # -----------------------------------------------------
        st.write("### Toxicity-Vergleich zwischen Gruppen")

        fig2, ax2 = plt.subplots()
        ax2.bar(df_bias["Gruppe"], df_bias["Toxicity (√ò)"])
        ax2.set_xlabel("Gruppe")
        ax2.set_ylabel("Durchschnittliche Toxicity")
        st.pyplot(fig2)



        # -----------------------------------------------------
        # Bias Score Anzeige
        # -----------------------------------------------------
        st.subheader("Bias-Score")

        st.write(f"**Bias Score:** {bias_scores['bias_score']:.2f}")
        st.write(f"- Sentiment-Gap: {bias_scores['sentiment_gap']:.2f}")
        st.write(f"- Toxicity-Gap: {bias_scores['toxicity_gap']:.2f}")

        if bias_scores["bias_score"] > 0.20:
            st.error("‚ö†Ô∏è Hinweis: Das Modell zeigt deutliche Unterschiede zwischen Gruppen.")
        else:
            st.success("‚úîÔ∏è Geringe Unterschiede zwischen den Gruppen.")



        # -----------------------------------------------------
        # Beispieltexte
        # -----------------------------------------------------
        st.subheader("Beispieltexte aus den Gruppen")

        for group, data in group_results.items():
            st.markdown(f"### {group}")

            with st.expander("üîé Beispieltexte anzeigen"):
                for t in data["examples"]:
                    st.markdown(f"> {t}")

            st.markdown("---")

    else:
        st.info("Klicke auf 'Text-Bias Analyse starten', um die Gruppen zu vergleichen.")
# ================================================================
# TAB 3: BILD-BIAS KATEGORIEN (KONZEPT)
# ================================================================
with tab3:
    st.header("Bild-Bias Kategorien")
    st.write(
        "Dieses Modul definiert die Kategorien, nach denen KI-generierte Bilder "
        "im Hinblick auf m√∂gliche Verzerrungen analysiert werden sollen. "
        "Die Kategorien sind ethisch und datenschutzsensibel formuliert."
    )

    categories_data = [
        {
            "Kategorie": "Gender Expression",
            "Beschreibung": "Sichtbare geschlechtliche Erscheinung basierend auf √§u√üeren Merkmalen.",
            "Messbare Auspr√§gungen": "maskulin erscheinend ¬∑ feminin erscheinend ¬∑ uneindeutig",
            "Ethischer Hinweis": "Keine Aussagen √ºber Identit√§t oder Zugeh√∂rigkeit, Trans-/Nonbinary-Identit√§t nicht erkennbar."
        },
        {
            "Kategorie": "Skin Tone Cluster",
            "Beschreibung": "Helligkeits- bzw. Farbcluster der Hautt√∂ne.",
            "Messbare Auspr√§gungen": "hell ¬∑ mittel ¬∑ dunkel",
            "Ethischer Hinweis": "Kein R√ºckschluss auf Ethnie, Nationalit√§t oder 'Rasse'."
        },
        {
            "Kategorie": "Age Appearance",
            "Beschreibung": "Alterseindruck basierend auf visuellen Merkmalen.",
            "Messbare Auspr√§gungen": "Kind ¬∑ Jugendliche Person ¬∑ Erwachsene Person ¬∑ √Ñltere Person",
            "Ethischer Hinweis": "Kein R√ºckschluss auf tats√§chliches Alter."
        },
        {
            "Kategorie": "Visible Assistive Attributes",
            "Beschreibung": "Sichtbare Hinweise auf Hilfsmittel oder Barrieren.",
            "Messbare Auspr√§gungen": "Hilfsmittel sichtbar (z. B. Rollstuhl, Prothese) ¬∑ keine Hilfsmittel sichtbar ¬∑ unklar",
            "Ethischer Hinweis": "Keine medizinischen Diagnosen oder Aussagen √ºber Behinderungsgrad."
        },
        {
            "Kategorie": "Clothing Style / Role Indicators",
            "Beschreibung": "Kleidungstypen, die soziale Rollen oder Berufe repr√§sentieren k√∂nnen.",
            "Messbare Auspr√§gungen": "Business-Outfit ¬∑ Freizeitkleidung ¬∑ Sportbekleidung ¬∑ Uniform/berufsspezifisch",
            "Ethischer Hinweis": "Es werden nur sichtbare Muster betrachtet, keine Stereotype festgeschrieben."
        },
        {
            "Kategorie": "Visible Religious Symbols",
            "Beschreibung": "Sichtbare Kleidungsst√ºcke oder Accessoires mit religi√∂ser Funktion.",
            "Messbare Auspr√§gungen": "Kopftuch ¬∑ Kippa ¬∑ Kreuzanh√§nger ¬∑ Turban ¬∑ keine sichtbaren Symbole ¬∑ unklar",
            "Ethischer Hinweis": "Es geht nur um sichtbare Objekte, nicht um religi√∂se Zugeh√∂rigkeit."
        },
        {
            "Kategorie": "Body Shape Appearance",
            "Beschreibung": "Wahrgenommene K√∂rperform basierend auf visuellen Merkmalen.",
            "Messbare Auspr√§gungen": "schlank ¬∑ durchschnittlich ¬∑ kr√§ftig ¬∑ plus-size erscheinend ¬∑ unklar",
            "Ethischer Hinweis": "Keine Bewertung, nur neutrale Beschreibung der Bilddarstellung."
        },
    ]

    df_categories = pd.DataFrame(categories_data)

    st.subheader("Kategorie-√úbersicht")
    st.dataframe(df_categories, use_container_width=True)

    st.markdown("---")
    st.info(
        "Hinweis: Diese Kategorien beschreiben ausschlie√ülich sichtbare Bildmerkmale. "
        "Es werden keine sensiblen personenbezogenen Daten im rechtlichen Sinne "
        "oder Identit√§ten im Hintergrund 'erraten', sondern nur visuelle Muster analysiert."
    )
# ================================================================
# TAB 4: BILD-BIAS ANALYSE
# ================================================================
with tab4:
    st.header("Bild-Bias Analyse")

    from src.image_bias_analysis import analyze_image_bias

    uploaded = st.file_uploader("Bild hochladen", type=["jpg", "jpeg", "png"])

    category = st.selectbox(
    "Kategorie ausw√§hlen",
    [
        "Skin Tone Cluster",
        "Gender Expression",
        "Age Appearance",
        "Clothing Style",
        "Visible Religious Symbols",
        "Body Shape Appearance"
    ]
    )


    if uploaded and st.button("Analyse starten"):
        st.image(uploaded, caption="Hochgeladenes Bild", width=300)

        results = analyze_image_bias(uploaded, category)

        st.subheader("Ergebnis")
        st.write(f"**Kategorie:** {results['category']}")
        st.write(f"**Erkannt als:** {results['result']}")
        st.write(f"**Konfidenz:** {results['confidence']:.2f}")

        st.markdown("### Erkl√§rung")
        st.info(results["explanation"])

        st.markdown("### Ethik-Hinweis")
        st.warning(results["ethical_note"])

        # ================================================================
# TAB 5: NLP BIAS SCANNER
# ================================================================
with tab5:
    from src.nlp_bias_detector import analyze_text_bias, CATEGORY_DESCRIPTIONS
    import pandas as pd

    st.header("NLP Bias Scanner")
    st.write(
        "Dieses Modul pr√ºft Texte auf potenziell diskriminierende oder "
        "ausschlie√üende Formulierungen (z. B. in Stellenanzeigen, "
        "Kommunikation oder Webtexten)."
    )

    example_text = (
        "Wir sind ein junges, dynamisches Team und suchen eine Sekret√§rin. "
        "Deutsch als Muttersprache ist erforderlich. "
        "Bitte nur Bewerbungen von deutschen Staatsb√ºrgern ohne Migrationshintergrund."
    )

    text_input = st.text_area(
        "Text zur Analyse",
        value=example_text,
        height=200,
    )

    if st.button("üîé Text auf Bias pr√ºfen"):
        if not text_input.strip():
            st.warning("Bitte gib einen Text ein.")
        else:
            results = analyze_text_bias(text_input)

            st.subheader("Gesamtbewertung")
            score = results["overall_score"]
            st.write(f"**Gesamt-Bias-Score:** {score:.2f} (0 = unauff√§llig, 1 = stark auff√§llig)")
            st.write(f"Anzahl gefundener kritischer Stellen: **{results['total_hits']}**")

            if score == 0:
                st.success("Keine der hinterlegten problematischen Formulierungen wurde gefunden.")
            elif score < 0.4:
                st.info("Einige problematische Formulierungen. Eine √úberarbeitung ist empfehlenswert.")
            else:
                st.error("Deutliche Hinweise auf diskriminierende oder ausschlie√üende Sprache.")

            # √úbersicht pro Kategorie
            st.subheader("Kategorien√ºbersicht")

            table_rows = []
            for cat, data in results["categories"].items():
                table_rows.append({
                    "Kategorie": cat,
                    "Beschreibung": CATEGORY_DESCRIPTIONS.get(cat, ""),
                    "Anzahl Treffer": data["n_hits"],
                })
            df_cat = pd.DataFrame(table_rows)
            st.dataframe(df_cat, use_container_width=True)

            # Detailansicht pro Kategorie
            st.subheader("Details nach Kategorie")

            for cat, data in results["categories"].items():
                if data["n_hits"] == 0:
                    continue

                with st.expander(f"{cat} ‚Äì {data['n_hits']} Treffer"):
                    for hit in data["hits"]:
                        st.markdown(f"- **Gefunden:** ‚Äû{hit['match']}‚Äú")
                        st.markdown(f"  - Erkl√§rung: {hit['explanation']}")
                        st.markdown(f"  - Vorschlag: _{hit['suggestion']}_")
    else:
        st.info("Gib einen Text ein oder nutze das Beispiel und klicke auf ‚ÄûText auf Bias pr√ºfen‚Äú.")        

# ================================================================
# TAB 6: EDGE AI ROBUSTNESS & BIAS SIMULATOR
# ================================================================
with tab6:
    st.header("Edge AI Robustness & Bias Simulator")
    st.write(
        "Dieser Simulator untersucht, wie sich Edge-KI-Bedingungen wie "
        "schlechte Beleuchtung und Rauschen auf die Erkennungsqualit√§t und "
        "potenzielle Bias-Effekte auswirken."
    )

    from src.edge_simulator import run_edge_simulations
    from src.edge_bias_analysis import analyze_edge_bias

    uploaded_edge = st.file_uploader(
        "Bild hochladen (f√ºr Edge-Simulationen)",
        type=["jpg", "jpeg", "png"]
    )

    if uploaded_edge:
        st.subheader("Originalbild")
        st.image(uploaded_edge, width=300)

        if st.button("üîé Edge-Simulationen ausf√ºhren"):
            st.info("Simuliere Edge-KI-Bedingungen‚Ä¶")

            # ---- EDGE SIMULATION AUSF√úHREN ----
            results = run_edge_simulations(uploaded_edge)

            # üåì Low Light anzeigen
            st.subheader("Low-Light Simulation (schlechte Beleuchtung)")
            st.image(
                results["low_light"],
                width=300,
                caption="Low-Light Version"
            )

            # üå´ Noise anzeigen
            st.subheader("Noise Simulation (Sensorrauschen)")
            st.image(
                results["noise"],
                width=300,
                caption="Rauschen / Sensorausfall"
            )

            # ================================
            # FAIRNESS- & ROBUSTHEITSANALYSE
            # ================================
            st.subheader("‚öñÔ∏è Fairness- & Robustheitsanalyse")

            bias_results = analyze_edge_bias(
                results["original"],
                results["low_light"],
                results["noise"]
            )

            # Scores
            low_light_score = bias_results["low_light"]["bias_impact"]
            noise_score = bias_results["noise"]["bias_impact"]

            st.write("### üìä Bias Impact Scores")
            st.write(f"**Low Light:** {low_light_score:.2f}")
            st.write(f"**Noise:** {noise_score:.2f}")

            # Ampelsystem
            def score_color(score):
                if score < 0.2:
                    return "üü¢ niedrig"
                elif score < 0.5:
                    return "üü° mittel"
                else:
                    return "üî¥ hoch"

            st.write("### üìâ Bewertung der Risiken")
            st.write(f"**Low-Light Risiko:** {score_color(low_light_score)}")
            st.write(f"**Noise Risiko:** {score_color(noise_score)}")

            # Kurzbericht
            st.subheader("üìÑ Kurzbericht")
            st.info(
                f"- Low-Light Bedingungen f√ºhren zu einem Bias-Impact von **{low_light_score:.2f}**.\n"
                f"- Noise Bedingungen f√ºhren zu einem Bias-Impact von **{noise_score:.2f}**.\n"
                f"- H√∂here Werte zeigen, dass das Modell unter Edge-Bedingungen weniger zuverl√§ssig und potenziell unfair arbeitet."
            )

    else:
        st.info("Bitte lade ein Bild hoch, um die Edge-Simulationen zu starten.")

# ================================================================
# TAB 7: AI Ethics Live Monitor (Webanalyse + Textanalyse)
# ================================================================
with tab7:
    st.header("AI Ethics Live Monitor (Webanalyse)")
    st.write(
        "Dieses Modul l√§dt den Text einer Webseite oder eines Eingabetextes "
        "und analysiert ihn hinsichtlich Toxicity (Moderation API) "
        "und Bias (GPT-Analyse)."
    )

    from src.web_text_extractor import get_clean_text_from_url
    from src.api_text_analyzer import analyze_moderation_long_text, analyze_bias_gpt

    # --- Auswahl: URL oder manueller Text
    mode = st.radio(
        "Analysemodus ausw√§hlen:",
        ["Webseite (URL)", "Manueller Text"],
        horizontal=True
    )

    text = ""

    if mode == "Webseite (URL)":
        url = st.text_input("Webseite zur Analyse (URL eingeben)")
        if st.button("üåê Webseite laden"):
            if url:
                try:
                    text = get_clean_text_from_url(url)
                    st.session_state.web_text_monitor = text
                    st.success("Webseite erfolgreich geladen.")
                except Exception as e:
                    st.error(f"Fehler beim Laden der Webseite: {e}")
            else:
                st.warning("Bitte gib eine g√ºltige URL ein.")

        # Text aus Session holen, falls schon geladen
        text = st.session_state.get("web_text_monitor", "")

    else:
        # manueller Text
        text = st.text_area(
            "Text zur Analyse eingeben:",
            value=st.session_state.get("manual_text_monitor", ""),
            height=200
        )
        st.session_state.manual_text_monitor = text

    if text:
        st.subheader("Extrahierter / zu analysierender Text")
        st.text_area("Text", value=text, height=200)

        if st.button("üß™ Bias & Toxicity Analyse starten"):
            with st.spinner("Analysiere Text mit OpenAI Moderation & GPT‚Ä¶"):

                # 1) Moderation-Analyse
                mod_results = analyze_moderation_long_text(text)

                # 2) GPT-Bias-Analyse
                gpt_results = analyze_bias_gpt(text)

            # -------------------------
            # Anzeige: Moderation API
            # -------------------------
            # --- Moderation API Ergebnisse anzeigen ---
            st.subheader("üìä Moderation API ‚Äì Safety Scores")

            st.write(f"Bl√∂cke analysiert: {mod_results['blocks']}")

            scores = mod_results["scores"]

            for category, vals in scores.items():
                st.write(
                f"**{category}:** "
                f"Avg = {vals['avg']:.3f}, "
                f"Max = {vals['max']:.3f}"
            )


            # -------------------------
            # Anzeige: GPT-Bias-Analyse
            # -------------------------
            st.subheader("üß† GPT Bias-Analyse")

            overall = gpt_results.get("overall_risk", "unknown")
            st.write(f"**Gesamtrisiko (Bias):** {overall}")

            dims = gpt_results.get("dimensions", {})

            for dim_name, dim_data in dims.items():
                risk = dim_data.get("risk", "unknown")
                examples = dim_data.get("examples", [])
                st.write(f"**{dim_name.capitalize()}** ‚Äì Risiko: {risk}")
                if examples:
                    for ex in examples:
                        st.write(f"‚Ä¢ {ex}")

            comments = gpt_results.get("comments", [])
            if comments:
                st.subheader("üìù Kommentare")
                for c in comments:
                    st.write(f"- {c}")
    else:
        st.info("Bitte eine Webseite laden oder Text eingeben, um die Analyse zu starten.")
